{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T15:32:04.547612Z",
     "start_time": "2019-04-10T15:31:23.517647Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchsample as ts\n",
    "from torchsample.transforms import RandomRotate\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T15:32:09.206463Z",
     "start_time": "2019-04-10T15:32:09.181447Z"
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, csv_files,transform):\n",
    "        super().__init__()\n",
    "        self.csv_files=csv_files\n",
    "        with open(csv_files[0], 'rb') as f:\n",
    "            X = np.array(pickle.load(f),dtype=np.float32)\n",
    "        \n",
    "        if csv_files[1] is not None:\n",
    "            with open(csv_files[1], 'rb') as g:\n",
    "                y = np.array(pickle.load(g))\n",
    "            le=LabelEncoder()\n",
    "            y_label=le.fit_transform(y)\n",
    "            self.y=y_label\n",
    "            self.le =le \n",
    "            \n",
    "        self.transform=transform\n",
    "        \n",
    "        self.X= X.reshape(-1,1,28,28)\n",
    "#         self.X= X.reshape(-1,1,28,28)\n",
    "#         print(np.mean(np.mean(self.X,axis=0)))\n",
    "              \n",
    "                \n",
    "    def get_class_count(self):\n",
    "        return len(self.le.classes_)\n",
    "    \n",
    "    def get_input_dim(self):\n",
    "        return self.X.shape[1]\n",
    "    \n",
    "    def get_class_labels(self,y_label):\n",
    "        le=self.le\n",
    "        return le.inverse_transform(y_label)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "         z=self.X[idx]\n",
    "         #print(z.shape)\n",
    "         z=np.reshape(z, (28,28))\n",
    "         img_name = Image.fromarray(z)\n",
    "#         plt.imshow(img_name)\n",
    "         sample=self.X[idx]\n",
    "        \n",
    "         if self.transform is not None:\n",
    "            sample = self.transform(img_name)\n",
    "#             #plt.show()\n",
    "            sample= sample.reshape(1,28,28)         \n",
    "            return sample,self.y[idx]\n",
    "         \n",
    "         return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T15:32:12.243719Z",
     "start_time": "2019-04-10T15:32:11.516401Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_size=0.3\n",
    "shuffle=True\n",
    "valid_transform = transforms.Compose([transforms.ToTensor(),  transforms.Normalize(mean=[82.34472], std=[255]),])\n",
    "\n",
    "augment_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomAffine(30), transforms.ToTensor(),  transforms.Normalize(mean=[82.34472], std=[255]),])\n",
    "train_transform = transforms.Compose([ transforms.ToTensor(),  transforms.Normalize(mean=[82.34472], std=[255]),])\n",
    "\n",
    "train_dataset = ImageDataset(['train_image.pkl','train_label.pkl'],train_transform)\n",
    "valid_dataset = ImageDataset(['train_image.pkl','train_label.pkl'],valid_transform)\n",
    "augment_dataset = ImageDataset(['train_image.pkl','train_label.pkl'],augment_transform)\n",
    "#test_dataset = ImageDataset(['test_image.pkl', None],transforms.ToTensor())\n",
    "batch_size=512\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "if shuffle:\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "augment_loader = torch.utils.data.DataLoader(augment_dataset, batch_size=batch_size, sampler=valid_sampler)\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T15:32:14.348422Z",
     "start_time": "2019-04-10T15:32:14.345419Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "n_iters = 3000\n",
    "num_epochs = n_iters / (len(train_dataset) / batch_size)\n",
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T15:32:17.414417Z",
     "start_time": "2019-04-10T15:32:17.389400Z"
    }
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.batch1=nn.BatchNorm2d(32)\n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.batch2=nn.BatchNorm2d(64)\n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "#         self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3)\n",
    "#         self.relu3 = nn.ReLU()\n",
    "#         self.batch3=nn.BatchNorm2d(64)\n",
    "#         # Max pool 3\n",
    "#         self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_features=64 * 4 * 4, out_features=60)\n",
    "        self.droput = nn.Dropout(p=0.5)\n",
    "        self.outer = nn.Linear(in_features=60, out_features=4)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # Convolution 1\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.batch1(out)\n",
    "        # Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "\n",
    "        # Convolution 2 \n",
    "        out = self.conv2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.batch2(out)\n",
    "        # Max pool 2 \n",
    "        out = self.maxpool2(out)\n",
    "        \n",
    "#         # Convolution 3 \n",
    "#         out = self.conv3(out)\n",
    "#         out = self.relu3(out)\n",
    "#         out = self.batch3(out)\n",
    "#         # Max pool 3 \n",
    "#         out = self.maxpool3(out)\n",
    "\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        # Linear function (readout)\n",
    "        out = self.fc1(out)\n",
    "        #out = self.fc2(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.outer(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-10T15:32:19.957428Z",
     "start_time": "2019-04-10T15:32:19.933399Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Network().cuda(1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-04-10T10:02:26.522Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0. Training_Loss: 1.2402558661997318.Testing_Loss:1.0449686527252198. Accuracy: 62\n",
      "Epoch: 2. Training_Loss: 0.6426720842719078.Testing_Loss:0.5758485913276672. Accuracy: 76\n",
      "Epoch: 4. Training_Loss: 0.5463480800390244.Testing_Loss:0.49000545144081115. Accuracy: 81\n",
      "Epoch: 6. Training_Loss: 0.4927375353872776.Testing_Loss:0.42074469923973085. Accuracy: 83\n",
      "Epoch: 8. Training_Loss: 0.45763013884425163.Testing_Loss:0.4233051002025604. Accuracy: 83\n",
      "Epoch: 10. Training_Loss: 0.4449487794190645.Testing_Loss:0.45953909158706663. Accuracy: 81\n",
      "Epoch: 12. Training_Loss: 0.41758232191205025.Testing_Loss:0.4133780479431152. Accuracy: 84\n",
      "Epoch: 14. Training_Loss: 0.44467348232865334.Testing_Loss:0.4002498209476471. Accuracy: 84\n",
      "Epoch: 16. Training_Loss: 0.478543184697628.Testing_Loss:0.44628652930259705. Accuracy: 82\n",
      "Epoch: 18. Training_Loss: 0.40339032001793385.Testing_Loss:0.48504101037979125. Accuracy: 82\n",
      "Epoch: 20. Training_Loss: 0.37640961445868015.Testing_Loss:0.3571521043777466. Accuracy: 86\n",
      "Epoch: 22. Training_Loss: 0.35116838198155165.Testing_Loss:0.4094533324241638. Accuracy: 84\n",
      "Epoch: 24. Training_Loss: 0.33375889994204044.Testing_Loss:0.36348252296447753. Accuracy: 86\n",
      "Epoch: 26. Training_Loss: 0.3113791774958372.Testing_Loss:0.349796986579895. Accuracy: 86\n",
      "Epoch: 28. Training_Loss: 0.3201513886451721.Testing_Loss:0.37449565529823303. Accuracy: 87\n",
      "Epoch: 30. Training_Loss: 0.3190763173624873.Testing_Loss:0.32133068740367887. Accuracy: 87\n",
      "Epoch: 32. Training_Loss: 0.3001041105017066.Testing_Loss:0.3339590847492218. Accuracy: 87\n",
      "Epoch: 34. Training_Loss: 0.2996823824942112.Testing_Loss:0.3145266532897949. Accuracy: 88\n",
      "Epoch: 36. Training_Loss: 0.26772697549313307.Testing_Loss:0.36625418066978455. Accuracy: 86\n",
      "Epoch: 38. Training_Loss: 0.22632091771811247.Testing_Loss:0.318097847700119. Accuracy: 87\n",
      "Epoch: 40. Training_Loss: 0.2094268542714417.Testing_Loss:0.2835650473833084. Accuracy: 89\n",
      "Epoch: 42. Training_Loss: 0.20989525644108653.Testing_Loss:0.28860711455345156. Accuracy: 88\n",
      "Epoch: 44. Training_Loss: 0.23227955680340528.Testing_Loss:0.2786278635263443. Accuracy: 88\n",
      "Epoch: 46. Training_Loss: 0.27029735781252384.Testing_Loss:0.28333376348018646. Accuracy: 88\n",
      "Epoch: 48. Training_Loss: 0.22727026557549834.Testing_Loss:0.2672090709209442. Accuracy: 90\n",
      "Epoch: 50. Training_Loss: 0.1966043822467327.Testing_Loss:0.3011720895767212. Accuracy: 88\n",
      "Epoch: 52. Training_Loss: 0.21441647689789534.Testing_Loss:0.2577683836221695. Accuracy: 90\n",
      "Epoch: 54. Training_Loss: 0.23845613515004516.Testing_Loss:0.2647783547639847. Accuracy: 89\n",
      "Epoch: 56. Training_Loss: 0.25282480334863067.Testing_Loss:0.2796711683273315. Accuracy: 89\n",
      "Epoch: 58. Training_Loss: 0.2115447954274714.Testing_Loss:0.2754540354013443. Accuracy: 89\n",
      "Epoch: 60. Training_Loss: 0.19209851836785674.Testing_Loss:0.28131639063358305. Accuracy: 89\n",
      "Epoch: 62. Training_Loss: 0.18330692499876022.Testing_Loss:0.3094907641410828. Accuracy: 89\n",
      "Epoch: 64. Training_Loss: 0.19277848931960762.Testing_Loss:0.2462583988904953. Accuracy: 91\n",
      "Epoch: 66. Training_Loss: 0.207627032417804.Testing_Loss:0.26630844473838805. Accuracy: 90\n",
      "Epoch: 68. Training_Loss: 0.178037793841213.Testing_Loss:0.2952010750770569. Accuracy: 90\n",
      "Epoch: 70. Training_Loss: 0.1921903695911169.Testing_Loss:0.31523788571357725. Accuracy: 89\n",
      "Epoch: 72. Training_Loss: 0.20679311221465468.Testing_Loss:0.3024976372718811. Accuracy: 89\n",
      "Epoch: 74. Training_Loss: 0.19371554092504084.Testing_Loss:0.2634598046541214. Accuracy: 90\n",
      "Epoch: 76. Training_Loss: 0.17810761090368032.Testing_Loss:0.24607574045658112. Accuracy: 90\n",
      "Epoch: 78. Training_Loss: 0.16779041476547718.Testing_Loss:0.32647532820701597. Accuracy: 88\n",
      "Epoch: 80. Training_Loss: 0.15519888699054718.Testing_Loss:0.24420492053031922. Accuracy: 90\n",
      "Epoch: 82. Training_Loss: 0.16978445160202682.Testing_Loss:0.21984703838825226. Accuracy: 91\n",
      "Epoch: 84. Training_Loss: 0.15241479803808033.Testing_Loss:0.2698522746562958. Accuracy: 90\n",
      "Epoch: 86. Training_Loss: 0.1534239905886352.Testing_Loss:0.21458599865436553. Accuracy: 91\n",
      "Epoch: 88. Training_Loss: 0.13745441706851125.Testing_Loss:0.22777531147003174. Accuracy: 92\n",
      "Epoch: 90. Training_Loss: 0.13900027482304722.Testing_Loss:0.2004971534013748. Accuracy: 91\n",
      "Epoch: 92. Training_Loss: 0.14950794773176312.Testing_Loss:0.21439381837844848. Accuracy: 92\n",
      "Epoch: 94. Training_Loss: 0.14210278750397265.Testing_Loss:0.20208337306976318. Accuracy: 93\n",
      "Epoch: 96. Training_Loss: 0.14562642411328852.Testing_Loss:0.1898043543100357. Accuracy: 93\n",
      "Epoch: 98. Training_Loss: 0.12558040767908096.Testing_Loss:0.21588764786720277. Accuracy: 91\n",
      "Epoch: 100. Training_Loss: 0.14591088984161615.Testing_Loss:0.19402450025081636. Accuracy: 92\n",
      "Epoch: 102. Training_Loss: 0.17923776269890368.Testing_Loss:0.22780444622039794. Accuracy: 92\n",
      "Epoch: 104. Training_Loss: 0.13787499233148992.Testing_Loss:0.22767381966114045. Accuracy: 91\n",
      "Epoch: 106. Training_Loss: 0.14359371433965862.Testing_Loss:0.1958857834339142. Accuracy: 92\n",
      "Epoch: 108. Training_Loss: 0.14042043255176395.Testing_Loss:0.18656249940395356. Accuracy: 93\n",
      "Epoch: 110. Training_Loss: 0.1513343311380595.Testing_Loss:0.20378781855106354. Accuracy: 92\n",
      "Epoch: 112. Training_Loss: 0.14317586785182357.Testing_Loss:0.17624911665916443. Accuracy: 93\n",
      "Epoch: 114. Training_Loss: 0.1516674750018865.Testing_Loss:0.1702099174261093. Accuracy: 93\n",
      "Epoch: 116. Training_Loss: 0.14096319826785475.Testing_Loss:0.18477618992328643. Accuracy: 93\n",
      "Epoch: 118. Training_Loss: 0.13117434561718255.Testing_Loss:0.19853819012641907. Accuracy: 93\n",
      "Epoch: 120. Training_Loss: 0.14369444525800645.Testing_Loss:0.17621295750141144. Accuracy: 94\n",
      "Epoch: 122. Training_Loss: 0.12755635648500174.Testing_Loss:0.23324071168899535. Accuracy: 91\n",
      "Epoch: 124. Training_Loss: 0.1366322215180844.Testing_Loss:0.18701922595500947. Accuracy: 93\n",
      "Epoch: 126. Training_Loss: 0.13423958886414766.Testing_Loss:0.16499201953411102. Accuracy: 94\n",
      "Epoch: 128. Training_Loss: 0.1179055463289842.Testing_Loss:0.1550823539495468. Accuracy: 94\n",
      "Epoch: 130. Training_Loss: 0.14033768873196095.Testing_Loss:0.24513320922851561. Accuracy: 91\n",
      "Epoch: 132. Training_Loss: 0.15314829163253307.Testing_Loss:0.19047505855560304. Accuracy: 92\n",
      "Epoch: 134. Training_Loss: 0.14290268521290272.Testing_Loss:0.1856757402420044. Accuracy: 93\n",
      "Epoch: 136. Training_Loss: 0.12850501586217433.Testing_Loss:0.15878133326768876. Accuracy: 94\n",
      "Epoch: 138. Training_Loss: 0.11878543475177139.Testing_Loss:0.16441585421562194. Accuracy: 94\n",
      "Epoch: 140. Training_Loss: 0.16050380293745548.Testing_Loss:0.19407618343830108. Accuracy: 92\n",
      "Epoch: 142. Training_Loss: 0.1345021401066333.Testing_Loss:0.16166438162326813. Accuracy: 94\n",
      "Epoch: 144. Training_Loss: 0.110018479404971.Testing_Loss:0.14791158139705657. Accuracy: 95\n",
      "Epoch: 146. Training_Loss: 0.11110691027715802.Testing_Loss:0.1970808684825897. Accuracy: 92\n",
      "Epoch: 148. Training_Loss: 0.13560660183429718.Testing_Loss:0.17451277673244475. Accuracy: 93\n",
      "Epoch: 150. Training_Loss: 0.115536849363707.Testing_Loss:0.18938775956630707. Accuracy: 93\n",
      "Epoch: 152. Training_Loss: 0.11708764615468681.Testing_Loss:0.14735511541366578. Accuracy: 93\n",
      "Epoch: 154. Training_Loss: 0.10623224219307303.Testing_Loss:0.14036500006914138. Accuracy: 94\n",
      "Epoch: 156. Training_Loss: 0.15548284503165632.Testing_Loss:0.16689347624778747. Accuracy: 93\n",
      "Epoch: 158. Training_Loss: 0.15218944079242647.Testing_Loss:0.18980571031570434. Accuracy: 92\n",
      "Epoch: 160. Training_Loss: 0.12443289742805064.Testing_Loss:0.17474094331264495. Accuracy: 93\n",
      "Epoch: 162. Training_Loss: 0.1268759509548545.Testing_Loss:0.14499205350875854. Accuracy: 93\n",
      "Epoch: 164. Training_Loss: 0.11099679418839514.Testing_Loss:0.12686622738838196. Accuracy: 94\n",
      "Epoch: 166. Training_Loss: 0.1007934627123177.Testing_Loss:0.14698738604784012. Accuracy: 94\n",
      "Epoch: 168. Training_Loss: 0.10727006196975708.Testing_Loss:0.13847867995500565. Accuracy: 94\n",
      "Epoch: 170. Training_Loss: 0.09924977592891082.Testing_Loss:0.12550176680088043. Accuracy: 95\n",
      "Epoch: 172. Training_Loss: 0.14110166823957115.Testing_Loss:0.17110865712165832. Accuracy: 93\n",
      "Epoch: 174. Training_Loss: 0.12630161794368178.Testing_Loss:0.15964995920658112. Accuracy: 94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 176. Training_Loss: 0.12146708637010306.Testing_Loss:0.18399595022201537. Accuracy: 94\n",
      "Epoch: 178. Training_Loss: 0.13465023133903742.Testing_Loss:0.2229388803243637. Accuracy: 92\n",
      "Epoch: 180. Training_Loss: 0.13289232295937836.Testing_Loss:0.21494647860527039. Accuracy: 93\n",
      "Epoch: 182. Training_Loss: 0.1209294879809022.Testing_Loss:0.19898196160793305. Accuracy: 93\n",
      "Epoch: 184. Training_Loss: 0.13254690438043326.Testing_Loss:0.21960621178150178. Accuracy: 93\n",
      "Epoch: 186. Training_Loss: 0.10529092245269567.Testing_Loss:0.18403094708919526. Accuracy: 93\n",
      "Epoch: 188. Training_Loss: 0.10721297923009843.Testing_Loss:0.15538381040096283. Accuracy: 94\n",
      "Epoch: 190. Training_Loss: 0.10916877922136337.Testing_Loss:0.1539050281047821. Accuracy: 94\n",
      "Epoch: 192. Training_Loss: 0.1106736371293664.Testing_Loss:0.18549111783504485. Accuracy: 94\n",
      "Epoch: 194. Training_Loss: 0.11098290979862213.Testing_Loss:0.17041124105453492. Accuracy: 94\n",
      "Epoch: 196. Training_Loss: 0.10131320159416646.Testing_Loss:0.12284425050020217. Accuracy: 96\n",
      "Epoch: 198. Training_Loss: 0.09487594559323043.Testing_Loss:0.14836927354335785. Accuracy: 94\n",
      "Epoch: 200. Training_Loss: 0.10109020606614649.Testing_Loss:0.15021738708019255. Accuracy: 94\n",
      "Epoch: 202. Training_Loss: 0.09994228085270151.Testing_Loss:0.13384179174900054. Accuracy: 95\n",
      "Epoch: 204. Training_Loss: 0.11505839368328452.Testing_Loss:0.13162878453731536. Accuracy: 95\n",
      "Epoch: 206. Training_Loss: 0.13222789717838168.Testing_Loss:0.17048570215702058. Accuracy: 93\n",
      "Epoch: 208. Training_Loss: 0.1047055769013241.Testing_Loss:0.141006001830101. Accuracy: 94\n",
      "Epoch: 210. Training_Loss: 0.09600668447092175.Testing_Loss:0.12143905013799668. Accuracy: 95\n",
      "Epoch: 212. Training_Loss: 0.12761351093649864.Testing_Loss:0.20938522219657899. Accuracy: 93\n",
      "Epoch: 214. Training_Loss: 0.12162126274779439.Testing_Loss:0.2034316211938858. Accuracy: 92\n",
      "Epoch: 216. Training_Loss: 0.1163295959122479.Testing_Loss:0.246344593167305. Accuracy: 92\n",
      "Epoch: 218. Training_Loss: 0.12647876888513565.Testing_Loss:0.1526592805981636. Accuracy: 94\n",
      "Epoch: 220. Training_Loss: 0.1105275607551448.Testing_Loss:0.15087540894746782. Accuracy: 94\n",
      "Epoch: 222. Training_Loss: 0.11708557570818812.Testing_Loss:0.13423681557178496. Accuracy: 94\n",
      "Epoch: 224. Training_Loss: 0.09763070184271783.Testing_Loss:0.1413726970553398. Accuracy: 95\n",
      "Epoch: 226. Training_Loss: 0.09656381706008688.Testing_Loss:0.15560946464538575. Accuracy: 94\n",
      "Epoch: 228. Training_Loss: 0.12341035093413666.Testing_Loss:0.16228193938732147. Accuracy: 94\n",
      "Epoch: 230. Training_Loss: 0.08866906329058111.Testing_Loss:0.12326281070709229. Accuracy: 95\n",
      "Epoch: 232. Training_Loss: 0.09466342834639363.Testing_Loss:0.0982274904847145. Accuracy: 96\n",
      "Epoch: 234. Training_Loss: 0.08847026561852545.Testing_Loss:0.11245535165071488. Accuracy: 96\n",
      "Epoch: 236. Training_Loss: 0.0801902114180848.Testing_Loss:0.1008748471736908. Accuracy: 95\n",
      "Epoch: 238. Training_Loss: 0.09189198951935396.Testing_Loss:0.1226888507604599. Accuracy: 95\n",
      "Epoch: 240. Training_Loss: 0.08707278422662057.Testing_Loss:0.11039485931396484. Accuracy: 96\n",
      "Epoch: 242. Training_Loss: 0.10003393900115043.Testing_Loss:0.14685279726982117. Accuracy: 95\n",
      "Epoch: 244. Training_Loss: 0.10369844984961674.Testing_Loss:0.12757004350423812. Accuracy: 95\n",
      "Epoch: 246. Training_Loss: 0.13458326866384596.Testing_Loss:0.14115914404392244. Accuracy: 95\n",
      "Epoch: 248. Training_Loss: 0.11474445683415979.Testing_Loss:0.19516007006168365. Accuracy: 94\n",
      "Epoch: 250. Training_Loss: 0.14070089935557917.Testing_Loss:0.14841263890266418. Accuracy: 94\n",
      "Epoch: 252. Training_Loss: 0.10894670081324875.Testing_Loss:0.23575014472007752. Accuracy: 93\n",
      "Epoch: 254. Training_Loss: 0.12128506373846903.Testing_Loss:0.19828312695026398. Accuracy: 93\n",
      "Epoch: 256. Training_Loss: 0.13252905814442784.Testing_Loss:0.2391201972961426. Accuracy: 92\n",
      "Epoch: 258. Training_Loss: 0.12286438071168959.Testing_Loss:0.25669765174388887. Accuracy: 92\n",
      "Epoch: 260. Training_Loss: 0.11719695577630773.Testing_Loss:0.15055084675550462. Accuracy: 94\n",
      "Epoch: 262. Training_Loss: 0.13042540138121694.Testing_Loss:0.29969478845596315. Accuracy: 90\n",
      "Epoch: 264. Training_Loss: 0.1217772223171778.Testing_Loss:0.24622247219085694. Accuracy: 92\n",
      "Epoch: 266. Training_Loss: 0.15044863265939057.Testing_Loss:0.3788015127182007. Accuracy: 89\n",
      "Epoch: 268. Training_Loss: 0.11193737282883376.Testing_Loss:0.16199003756046296. Accuracy: 94\n",
      "Epoch: 270. Training_Loss: 0.12310468638315797.Testing_Loss:0.21609224677085875. Accuracy: 93\n",
      "Epoch: 272. Training_Loss: 0.10063394327880815.Testing_Loss:0.14138913303613662. Accuracy: 94\n",
      "Epoch: 274. Training_Loss: 0.08583866970730014.Testing_Loss:0.1215976819396019. Accuracy: 95\n",
      "Epoch: 276. Training_Loss: 0.08802714908961207.Testing_Loss:0.11644662916660309. Accuracy: 96\n",
      "Epoch: 278. Training_Loss: 0.0718132479232736.Testing_Loss:0.1743098109960556. Accuracy: 94\n",
      "Epoch: 280. Training_Loss: 0.08033874342800118.Testing_Loss:0.13361017405986786. Accuracy: 95\n",
      "Epoch: 282. Training_Loss: 0.0890572628704831.Testing_Loss:0.12019108086824418. Accuracy: 95\n",
      "Epoch: 284. Training_Loss: 0.11560866021318361.Testing_Loss:0.1451214447617531. Accuracy: 94\n",
      "Epoch: 286. Training_Loss: 0.10389264079276472.Testing_Loss:0.23274458944797516. Accuracy: 92\n",
      "Epoch: 288. Training_Loss: 0.09018584410659969.Testing_Loss:0.10885500609874725. Accuracy: 95\n",
      "Epoch: 290. Training_Loss: 0.08867990504950285.Testing_Loss:0.11028213053941727. Accuracy: 96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-f37fc124099d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[1;31m# Updating parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mtraining_loss_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[0mtraining_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_loss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_loss_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m     98\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 100\u001b[1;33m                     \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[0mbias_correction1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mbeta1\u001b[0m \u001b[1;33m**\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "iter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    training_loss_list = []\n",
    "    testing_loss_list = []\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.requires_grad_().cuda(1)\n",
    "        labels =labels.cuda(1)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        training_loss_list.append(loss.item())\n",
    "    for i, (images, labels) in enumerate(augment_loader):\n",
    "    # Load images as tensors with gradient accumulation abilities\n",
    "        images = images.requires_grad_().cuda(1)\n",
    "        labels =labels.cuda(1)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Calculate Loss: softmax --> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Updating parameters\n",
    "        optimizer.step()\n",
    "        training_loss_list.append(loss.item())\n",
    "    training_loss = sum(training_loss_list)/len(training_loss_list)    \n",
    "\n",
    "    model.eval()    \n",
    "    if(epoch%2==0):\n",
    "            # Calculate Accuracy         \n",
    "        correct = 0\n",
    "        total = 0\n",
    "            # Iterate through test dataset\n",
    "        losses = []\n",
    "        for images, labels in validation_loader:\n",
    "            # Load images to tensors with gradient accumulation abilities\n",
    "            images = images.requires_grad_().cuda(1)\n",
    "            labels = labels.cuda(1)\n",
    "\n",
    "            # Forward pass only to get logits/output\n",
    "            outputs = model(images)\n",
    "            \n",
    "            loss = criterion(outputs, labels)\n",
    "            testing_loss_list.append(loss.item())\n",
    "            # Get predictions from the maximum value\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            # Total number of labels\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # Total correct predictions\n",
    "            correct += (predicted == labels).sum()\n",
    "\n",
    "            accuracy = 100 * correct / total\n",
    "        test_loss = sum(testing_loss_list)/len(testing_loss_list)\n",
    "        torch.save(model.state_dict(),str(epoch))\n",
    "        print('Epoch: {}. Training_Loss: {}.Testing_Loss:{}. Accuracy: {}'.format(epoch, training_loss,test_loss ,accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adarsh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 1, 1, 1, 2, 0, 1, 3, 0, 1, 2, 3, 0, 1, 0, 1, 3, 2, 1, 0, 1, 0, 1,\n",
      "        2, 0, 2, 3, 0, 1, 0, 1, 3, 0, 3, 0, 3, 1, 0, 2, 1, 2, 2, 0, 1, 2, 3, 2,\n",
      "        0, 3, 3, 2, 3, 3, 3, 0, 2, 0, 2, 0, 2, 0, 2, 3, 1, 1, 0, 0, 0, 3, 0, 0,\n",
      "        1, 1, 3, 2, 3, 3, 0, 1, 3, 2, 1, 3, 3, 3, 1, 2, 2, 2, 3, 2, 0, 3, 3, 1,\n",
      "        2, 3, 2, 0, 0, 0, 3, 3, 1, 3, 1, 1, 3, 0, 0, 0, 1, 1, 1, 3, 2, 3, 0, 1,\n",
      "        3, 2, 0, 3, 2, 1, 0, 2, 2, 2, 1, 2, 2, 0, 3, 2, 0, 1, 2, 3, 0, 0, 2, 3,\n",
      "        2, 3, 2, 2, 1, 2, 3, 0, 3, 3, 1, 1, 0, 2, 0, 1, 3, 2, 3, 2, 0, 0, 0, 2,\n",
      "        2, 1, 0, 1, 1, 0, 2, 1, 0, 3, 1, 0, 1, 3, 2, 1, 1, 0, 0, 3, 1, 1, 2, 1,\n",
      "        1, 3, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 2, 0, 2, 0, 2, 1,\n",
      "        0, 2, 0, 3, 0, 0, 2, 0, 1, 2, 0, 0, 3, 1, 1, 1, 1, 2, 0, 0, 3, 2, 0, 1,\n",
      "        3, 0, 3, 2, 2, 2, 1, 3, 1, 3, 3, 0, 0, 1, 2, 0, 3, 0, 0, 3, 1, 3, 1, 3,\n",
      "        1, 1, 1, 2, 2, 2, 0, 0, 3, 0, 2, 0, 1, 2, 3, 2, 3, 1, 3, 1, 0, 2, 0, 3,\n",
      "        2, 2, 3, 2, 1, 2, 3, 3, 3, 2, 2, 3, 0, 2, 0, 0, 1, 0, 3, 3, 0, 0, 2, 0,\n",
      "        0, 2, 0, 1, 3, 1, 2, 3, 1, 0, 0, 1, 0, 2, 1, 3, 2, 3, 1, 3, 1, 0, 2, 1,\n",
      "        2, 0, 1, 2, 0, 0, 0, 2, 0, 0, 2, 0, 2, 0, 2, 1], device='cuda:1')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Adarsh\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open('test_image.pkl', 'rb') as f:\n",
    "    X = np.array(pickle.load(f),dtype=np.float32)\n",
    "y = F.softmax(model(images))\n",
    "_, predicted = torch.max(y.data, 1)\n",
    "print(predicted)\n",
    "# access Variable's tensor, copy back to CPU, convert to numpy\n",
    "arr = predicted.data.cpu().numpy()\n",
    "arr=np.array(arr,dtype='uint8')\n",
    "final=train_dataset.get_class_labels(arr)\n",
    "# write CSV\n",
    "np.savetxt('output.csv', final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
